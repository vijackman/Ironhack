{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Parallelization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the parallelization lesson, we saw two examples of how we could implement parallelization using the multiprocessing library to speed up the tasks that we need to perform.\n",
    "\n",
    "This lab will combine parallelization with some of the other topics you have learned in the Intermediate Python module of this program (list comprehensions, requests library, functional programming, web scraping, etc.). You will write code that extracts a list of links from a web page, requests each URL, and then indexes the page referenced by each link - both sequentially and in parallel.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Open the `main.ipynb` file in the `your-code` directory. There are a bunch of questions to be solved. If you get stuck in one exercise you can skip to the next one. Read each instruction carefully and provide your answer beneath it.\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- `main.ipynb` with your responses to each of the exercises.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Upon completion, add your deliverables to git. Then commit git and push your branch to the remote.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Multiprocessing Library Documentation](https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing#module-multiprocessing)\n",
    "- [Python Parallel Computing (in 60 Seconds or less)](https://dbader.org/blog/python-parallel-computing-in-60-seconds)\n",
    "- [Python Multiprocessing: Pool vs Process â€“ Comparative Analysis](https://www.ellicium.com/python-multiprocessing-pool-process/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLBUDdX8SEjl"
   },
   "source": [
    "# Parallelization Lab\n",
    "\n",
    "In this lab, you will be leveraging several concepts you have learned to obtain a list of links from a web page and crawl and index the pages referenced by those links - both sequentially and in parallel. Follow the steps below to complete the lab.\n",
    "\n",
    "### Step 1: Use the requests library to retrieve the content from the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HlQZvSUqSEjn"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Data_science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHlgfatrSEjs"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ig5G0TTDSEjw"
   },
   "source": [
    "### Step 2: Use BeautifulSoup to extract a list of all the unique links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zahpf2s-SEjx"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Data_science'\n",
    "html = requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfmrZsypSEj0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "sys.path.insert(0, '/Users/victorjackman/Desktop/Ironhack/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/victorjackman/Desktop/Ironhack/',\n",
       " 'C:\\\\Users\\\\Victor Sampaio\\\\Desktop\\\\Ironhack\\\\Labs\\\\Parallelization\\\\your-code',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\python37.zip',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\DLLs',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3',\n",
       " '',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\Victor Sampaio\\\\.ipython']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7TXSNXCSEj3"
   },
   "source": [
    "### Step 3: Use list comprehensions with conditions to clean the link list.\n",
    "\n",
    "There are two types of links, absolute and relative. Absolute links have the full URL and begin with *http* while relative links begin with a forward slash (/) and point to an internal page within the *wikipedia.org* domain. Clean the respective types of URLs as follows.\n",
    "\n",
    "- Absolute Links: Create a list of these and remove any that contain a percentage sign (%).\n",
    "- Relative Links: Create a list of these, add the domain to the link so that you have the full URL, and remove any that contain a percentage sign (%).\n",
    "- Combine the list of absolute and relative links and ensure there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZqaDQ4gSEj4"
   },
   "outputs": [],
   "source": [
    "domain = 'http://wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHDkpTrISEj7"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liPZ-FYLSEkA"
   },
   "source": [
    "### Step 4: Use the os library to create a folder called *wikipedia* and make that the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5I-1EAcSEkA"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jErdx2cdSEkD"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qyDm2n-rSEkG"
   },
   "source": [
    "### Step 5: Write a function called index_page that accepts a link and does the following.\n",
    "\n",
    "- Tries to request the content of the page referenced by that link.\n",
    "- Slugifies the filename using the `slugify` function from the [python-slugify](https://pypi.org/project/python-slugify/) library and adds a .html file extension.\n",
    "    - If you don't already have the python-slugify library installed, you can pip install it as follows: `$ pip3 install python-slugify`.\n",
    "    - To import the slugify function, you would do the following: `from slugify import slugify`.\n",
    "    - You can then slugify a link as follows `slugify(link)`.\n",
    "- Creates a file in the wikipedia folder using the slugified filename and writes the contents of the page to the file.\n",
    "- If an exception occurs during the process above, just `pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTzUHCpBSEkH"
   },
   "outputs": [],
   "source": [
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mm7-pzw4SEkK"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VVDNEdyVSEkM"
   },
   "source": [
    "### Step 6: Sequentially loop through the list of links, running the index_page function each time.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run. \n",
    "\n",
    "_hint: Use tqdm to keep track of the time._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1akpUc5_SEkN"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWa8qXs3SEkQ"
   },
   "source": [
    "### Step 7: Perform the page indexing in parallel and note the difference in performance.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run.\n",
    "\n",
    "Use both methods, i.e., for one hand use the `multiprocess` module to use the function created in the jupyter notebook and run the download in parallel.\n",
    "\n",
    "And for another hand create a python file containing the function to download the file and use the `multiprocessing` module to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSA-oQV9SEkT"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Sx1hLdbTEp5"
   },
   "source": [
    "**BONUS**: Create a function that counts how many files are there in the wikipedia folder using the `os` module. \n",
    "\n",
    "Delete the files from the folder before you run and perform the above solution asynchronously. \n",
    "\n",
    "Use your function to check how many files are being downloaded."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
